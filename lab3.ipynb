{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class_map = {\"Happy\": 0,\n",
    "             \"Sad\": 1,\n",
    "             \"Surprised\": 2,\n",
    "             \"Mad\": 3}\n",
    "NUM_CLASSES = 4\n",
    "HOST_NAME = \"wildstylez\"\n",
    "VERSION_STR = \"1.0\"\n",
    "\n",
    "# You may have to change these two\n",
    "n_epochs = 10000\n",
    "batch_size = 64\n",
    "IMAGE_SHAPE = 64 # Images will be reshaped to IMAGE_SHAPE x IMAGE_SHAPE\n",
    "NUM_PIXEL_CHANNELS = 3\n",
    "INPUT_SHAPE = (IMAGE_SHAPE, IMAGE_SHAPE, NUM_PIXEL_CHANNELS)\n",
    "LATENT_DIM = 100\n",
    "\n",
    "\n",
    "if False:\n",
    "    # Set seeds for reproducibility\n",
    "    SEED = 2024\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(\"GPU(s) available.\")\n",
    "else:\n",
    "    print(\"No GPU(s) available.\")\n",
    "\n",
    "PLOT_SETTINGS = {\"text.usetex\": True,\n",
    "                 \"font.family\": \"serif\",\n",
    "                 \"figure.figsize\": (8.0, 6.0),\n",
    "                 \"font.size\": 16,\n",
    "                 \"axes.labelsize\": 16,\n",
    "                 \"legend.fontsize\": 14,\n",
    "                 \"xtick.labelsize\": 14,\n",
    "                 \"ytick.labelsize\": 14,\n",
    "                 \"axes.titlesize\": 24,\n",
    "                 \"lines.linewidth\": 2.0,\n",
    "                 }\n",
    "plt.rcParams.update(PLOT_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(keras.utils.Sequence):\n",
    "    \"\"\"A simple data loader for Assignment 1.\n",
    "\n",
    "    Note: Do not make changes to the data loader!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 class_map,\n",
    "                 batch_size=32,\n",
    "                 cache=True,\n",
    "                 random_state=None,\n",
    "                 dtype=np.uint8,\n",
    "                 ):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.class_map = class_map\n",
    "        self.batch_size = max(1, int(batch_size))\n",
    "        self.cache = bool(cache)\n",
    "        if random_state is None:\n",
    "            self.random_state = np.random\n",
    "        elif isinstance(random_state, np.random.RandomState):\n",
    "            self.random_state = random_state\n",
    "        else:\n",
    "            self.random_state = np.random.RandomState(random_state)\n",
    "        self.dtype = dtype\n",
    "\n",
    "        if self.data_path is None:\n",
    "            raise ValueError('The data path is not defined.')\n",
    "\n",
    "        if not os.path.isdir(self.data_path):\n",
    "            raise ValueError('The data path is incorrectly defined.')\n",
    "\n",
    "        if not isinstance(self.class_map, dict):\n",
    "            raise ValueError('The folder map is not a dictionary.')\n",
    "\n",
    "        # Read the files in all subfolders\n",
    "        self._file_idx = 0\n",
    "        self._images = []\n",
    "        self._labels = []\n",
    "        for folder in self.class_map:\n",
    "            path = os.path.join(self.data_path, folder)\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                self._images.append(file_path)\n",
    "                self._labels.append(folder)\n",
    "\n",
    "        self._image_cache = dict()\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of mini-batches per epoch.\"\"\"\n",
    "        return int(len(self._images) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get one batch of data.\"\"\"\n",
    "        # Generate indices of the batch\n",
    "        indices = self._indices[\n",
    "            index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find the next set of file indices\n",
    "        minibatch_files = [self._images[k] for k in indices]\n",
    "        minibatch_labels = [self.class_map[self._labels[k]] for k in indices]\n",
    "\n",
    "        # Load up the corresponding minibatch\n",
    "        minibatch = self.__load_minibatch(minibatch_files)\n",
    "\n",
    "        return minibatch, minibatch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Update indices after each epoch.\"\"\"\n",
    "        self._indices = np.arange(len(self._images))\n",
    "        self.random_state.shuffle(self._indices)\n",
    "\n",
    "    def __load_image(self, file):\n",
    "        \"\"\"Load a single image from file.\"\"\"\n",
    "        im = Image.open(file)\n",
    "        if im.mode != \"RGB\":\n",
    "            im = im.convert(\"RGB\")\n",
    "        \n",
    "        im = im.resize((IMAGE_SHAPE, IMAGE_SHAPE), Image.ANTIALIAS)\n",
    "        im = (np.asarray(im, dtype=self.dtype) / 255.0) * 2.0 - 1.0\n",
    "        \n",
    "        return im\n",
    "\n",
    "    def __load_minibatch(self, minibatch_files):\n",
    "        \"\"\"Load the next minibatch of samples.\"\"\"\n",
    "\n",
    "        try:\n",
    "            assert self.batch_size == len(minibatch_files)\n",
    "        except AssertionError:\n",
    "            print(self.batch_size)\n",
    "            print(len(minibatch_files))\n",
    "\n",
    "        minibatch = [None] * self.batch_size\n",
    "        for i, file in enumerate(minibatch_files):\n",
    "            if self.cache:\n",
    "                if file in self._image_cache:\n",
    "                    im = self._image_cache[file]\n",
    "                else:\n",
    "                    im = np.asarray(Image.open(file))\n",
    "                    im = self.__load_image(file)\n",
    "                    self._image_cache[file] = im\n",
    "            else:\n",
    "                im = self.__load_image(file)\n",
    "\n",
    "            minibatch[i] = im\n",
    "\n",
    "        return minibatch\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "# Directory where the data are\n",
    "data_dir = \"/import/software/5dv236/vt24/AffectNet/\"\n",
    "print(f\"Loading data from {data_dir}\")\n",
    "cache = True\n",
    "\n",
    "# Create the data loaders\n",
    "train_ds = DataLoader(os.path.join(data_dir, \"train/\"),\n",
    "                      class_map=class_map,\n",
    "                      batch_size=batch_size,\n",
    "                      cache=cache,\n",
    "                      )\n",
    "val_ds = DataLoader(os.path.join(data_dir, \"val/\"),\n",
    "                    class_map=class_map,\n",
    "                    batch_size=batch_size,\n",
    "                    cache=cache,\n",
    "                    )\n",
    "test_ds = DataLoader(os.path.join(data_dir, \"test/\"),\n",
    "                     class_map=class_map,\n",
    "                     batch_size=batch_size,\n",
    "                     cache=cache,\n",
    "                     )\n",
    "\n",
    "all_data_sets = [train_ds, val_ds, test_ds]\n",
    "\n",
    "# A quick summary of the data:\n",
    "print(f\"Number of training mini-batches: {len(train_ds)}\") # {len(train_ds)+len(val_ds)+len(test_ds)}\n",
    "print(f\"Number of training images      : {len(train_ds._indices)}\") # len(train_ds._indices)+len(val_ds._indices)+len(test_ds._indices)\n",
    "\n",
    "print(f\"\\nImage shape: {np.array(train_ds[0][0][0]).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few of the training images\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.subplots_adjust(top=0.995,\n",
    "                    bottom=0.005,\n",
    "                    left=0.025,\n",
    "                    right=0.995,\n",
    "                    wspace=0.05,\n",
    "                    hspace=0.0125)\n",
    "M, N = 4, 10\n",
    "axs = []\n",
    "for m in range(M):\n",
    "    axs.append([])\n",
    "    for n in range(N):\n",
    "        ax = plt.subplot2grid((M, N), (m, n), rowspan=1, colspan=1)\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_ticks([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticks([])\n",
    "        axs[m].append(ax)\n",
    "\n",
    "imgs = []\n",
    "lbls = []\n",
    "for epoch in range(3):\n",
    "    imgs.extend( ((np.array(train_ds[epoch][0]) + 1) / 2) * 255.0)\n",
    "    #imgs.extend((np.array(train_ds[epoch][0]) * 255.0))\n",
    "    lbls.extend(train_ds[epoch][1])\n",
    "indices = [0] * 4\n",
    "for epoch in range(len(imgs)):\n",
    "    y = lbls[epoch]\n",
    "    if indices[y] < N:\n",
    "        axs[y][indices[y]].imshow(imgs[epoch].astype(int)) # int [0,...,255]\n",
    "        indices[y] += 1\n",
    "for m in range(M):\n",
    "    label = list(train_ds.class_map.keys())[\n",
    "        list(train_ds.class_map.values()).index(m)]\n",
    "    axs[m][0].set_ylabel(f\"{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator():\n",
    "    in_label = tf.keras.layers.Input(shape=(1,))\n",
    "    li = tf.keras.layers.Embedding(NUM_CLASSES, 50)(in_label)\n",
    "    li = tf.keras.layers.Dense(IMAGE_SHAPE * IMAGE_SHAPE * NUM_PIXEL_CHANNELS)(li)\n",
    "    li = tf.keras.layers.Reshape(INPUT_SHAPE)(li)\n",
    "    \n",
    "    in_image = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "    merge = tf.keras.layers.Concatenate()([in_image, li])\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model([in_image, in_label], out_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def generator():\n",
    "    in_label = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "    foundation_dim = 8\n",
    "    li = tf.keras.layers.Embedding(NUM_CLASSES, 50)(in_label)\n",
    "    li = tf.keras.layers.Dense(foundation_dim*foundation_dim*NUM_PIXEL_CHANNELS)(li)\n",
    "    li = tf.keras.layers.Reshape((foundation_dim, foundation_dim, NUM_PIXEL_CHANNELS))(li)\n",
    "    \n",
    "    in_lat = tf.keras.layers.Input(shape=(LATENT_DIM,))\n",
    "    \n",
    "    # foundation for 8x8\n",
    "    x = tf.keras.layers.Dense(128 * foundation_dim * foundation_dim)(in_lat)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Reshape((foundation_dim, foundation_dim, 128))(x)\n",
    "\n",
    "    merge = tf.keras.layers.Concatenate()([x, li])\n",
    "\n",
    "    # upsample to 16x16\n",
    "    x = tf.keras.layers.Conv2DTranspose(512, (5,5), strides=(2,2), padding='same')(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # upsample to 32x32\n",
    "    x = tf.keras.layers.Conv2DTranspose(256, (5,5), strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    # upsample to 64x64\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    out_layer = tf.keras.layers.Conv2DTranspose(NUM_PIXEL_CHANNELS, (5,5), activation='tanh', padding='same')(x)\n",
    "    model = tf.keras.models.Model([in_lat, in_label], out_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def gan(generator_model, discriminator_model):\n",
    "    discriminator_model.trainable = False # freez discirminator\n",
    "\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_label = generator_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = generator_model.output\n",
    "\n",
    "    gan_output = discriminator_model([gen_output, gen_label])\n",
    "    \n",
    "    model = tf.keras.models.Model([gen_noise, gen_label], gan_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_latent_batch(number_of_samples):\n",
    "    return [np.random.randn(number_of_samples, LATENT_DIM).reshape(number_of_samples, LATENT_DIM), np.random.randint(0, NUM_CLASSES, number_of_samples)]\n",
    "\n",
    "\n",
    "def generate_fake_batch(generator_model, number_of_samples):\n",
    "    x_input, x_labels = get_latent_batch(number_of_samples)\n",
    "    X = generator_model.predict([x_input, x_labels])\n",
    "    y = np.zeros((number_of_samples, 1))\n",
    "    return [X, x_labels], y\n",
    "\n",
    "\n",
    "def save_plot(images, epoch, n=8):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        image = images[i, :, :, :]\n",
    "        image = (image + 1) / 2.0\n",
    "        plt.imshow(image)\n",
    "    # save plot to file\n",
    "    file_path = f\"./imgs/{VERSION_STR}_{HOST_NAME}_cGAN_generated_plot_{epoch+1:03d}.png\"\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    " \n",
    "\n",
    "def summarize_performance(epoch, gen_model, disc_model, gan_model, X_real, y_real):\n",
    "    _, acc_real = disc_model.evaluate([np.array(X_real), np.array(y_real)], np.ones((batch_size, 1)), verbose=0)\n",
    "    \n",
    "    [X_fake, X_fake_labels], y_fake = generate_fake_batch(gen_model, batch_size)\n",
    "\n",
    "    _, acc_fake = disc_model.evaluate([X_fake, X_fake_labels], y_fake, verbose=0)\n",
    "    \n",
    "    print(f\"Accuracy real: {acc_real*100:.0f}%, fake: {acc_fake*100:.0f}%\")\n",
    "    \n",
    "    save_plot(X_fake, epoch)\n",
    "\n",
    "    if True:\n",
    "        # Save model and waights\n",
    "        start_epoch = 1\n",
    "        gen_model.save(f\"./models/{VERSION_STR}_{HOST_NAME}_cGAN_generator_model_{epoch+start_epoch}.h5\")\n",
    "        \n",
    "        gen_model.save_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_generator_checkpoint_{epoch+start_epoch}.h5\")\n",
    "        disc_model.save_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_discriminator_checkpoint_{epoch+start_epoch}.h5\")\n",
    "        gan_model.save_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_gan_checkpoint_{epoch+start_epoch}.h5\")\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "########################################### TRAINING ###########################################\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "VERBOSE_LEVEL = 0\n",
    "LOAD_MODEL = False\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "\n",
    "gen_model = generator()\n",
    "disc_model = discriminator()\n",
    "gan_model = gan(gen_model, disc_model)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        checkpoint_num = 0\n",
    "        gen_model.load_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_generator_checkpoint_{checkpoint_num}.h5\")\n",
    "        disc_model.load_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_discriminator_checkpoint_{checkpoint_num}.h5\")\n",
    "        gan_model.load_weights(f\"./checkpoints/{VERSION_STR}_{HOST_NAME}_cGAN_gan_checkpoint_{checkpoint_num}.h5\")\n",
    "        print(\"Weights loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading weights:\", e)\n",
    "\n",
    "if VERBOSE_LEVEL > 0:\n",
    "    gen_model.summary()\n",
    "    disc_model.summary()\n",
    "    gan_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "batch_per_epoch = int(len(train_ds._indices) / batch_size)\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_number, (X_real, y_real) in enumerate(train_ds):\n",
    "        # Train discriminator on real images\n",
    "        disc_loss_real, _ = disc_model.train_on_batch([np.array(X_real), np.array(y_real)], np.ones((batch_size, 1)))\n",
    "        # Train discriminator on fake images\n",
    "        [X_fake, X_fake_labels], y_fake = generate_fake_batch(gen_model, batch_size)\n",
    "        d_loss_fake, _ = disc_model.train_on_batch([X_fake, X_fake_labels], np.zeros((batch_size, 1)))\n",
    "        \n",
    "        # Train cGAN model with latent points\n",
    "        [X_gan, X_gan_labels] = get_latent_batch(batch_size)\n",
    "        g_loss = gan_model.train_on_batch([X_gan, X_gan_labels], np.ones((batch_size, 1)))\n",
    "    \n",
    "        print(f\"Epoch: {epoch+1}/{n_epochs}, Batch: {batch_number+1}/{batch_per_epoch}, Discriminator loss: {(disc_loss_real+d_loss_fake)*0.5}, Generator loss: {g_loss}\")\n",
    "    \n",
    "    train_ds.on_epoch_end()\n",
    "    if (epoch+1) % SAVE_INTERVAL == 0:\n",
    "        summarize_performance(epoch, gen_model, disc_model, gan_model, X_real, y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"./model_versions/1000_v1/generator_model_1000.h5\"\n",
    "model_path = \"./models/cGAN_generator_model_1300.h5\"\n",
    "gen_model = load_model(model_path)\n",
    "\n",
    "\n",
    "def plot_batch(images, n=10):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        image = images[i, :, :, :]\n",
    "        image = (image + 1) / 2.0\n",
    "        plt.imshow(image)\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "num_images = 3\n",
    "x_input, x_labels = get_latent_batch(num_images)\n",
    "x_labels[0] = class_map[\"Mad\"]\n",
    "\n",
    "X = gen_model.predict([x_input, x_labels])\n",
    "plot_batch(X, num_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
